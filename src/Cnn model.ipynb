{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ad51745-e1a3-450c-8d8b-1f55675716d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "1%\n",
      "WARNING:tensorflow:From C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Loaded model from disk\n",
      "WARNING:tensorflow:From C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "25/25 [==============================] - 2s 59ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from loadData import loadDataset\n",
    "from loadModel import loadModel\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from subSetMaker import subSetMaker\n",
    "from frontend import createGUI\n",
    "import os\n",
    "import pickle as pkl\n",
    "\n",
    "#DON'T TOUCH, I DON'T KNOW WHAT IT DOES##\n",
    "from keras import backend as K         ##\n",
    "K.set_image_data_format('channels_first')       ##\n",
    "#########################################\n",
    "\n",
    "# Lấy đường dẫn của thư mục làm việc hiện tại\n",
    "PATH = os.getcwd()\n",
    "\n",
    "def baseline_model():\n",
    "    # Tạo một mô hình tuần tự (sequential)\n",
    "    model = Sequential()\n",
    "\n",
    "    # Thêm một lớp Convolutional 2D với 50 bộ lọc, mỗi bộ lọc kích thước (5, 5)\n",
    "    # Kích thước đầu vào là (1, 84, 83) với định dạng dữ liệu 'channels_first'\n",
    "    # Hàm kích hoạt là 'relu'\n",
    "    model.add(Conv2D(50, (5, 5), input_shape=(1, 84, 83), data_format='channels_first', activation='relu'))\n",
    "\n",
    "    # Thêm một lớp MaxPooling2D để giảm kích thước của đầu ra (pooling 2x2)\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Thêm một lớp Flatten để chuyển đổi đầu ra thành một vector phẳng\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Thêm một lớp Dense (fully connected) với 128 đơn vị và hàm kích hoạt 'relu'\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "\n",
    "    # Thêm một lớp Dense cuối cùng với số lượng đơn vị bằng số lớp và hàm kích hoạt 'softmax'\n",
    "    # Lớp này sẽ đưa ra xác suất dự đoán cho mỗi lớp\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Biên dịch mô hình với hàm mất mát là categorical cross-entropy\n",
    "    # Trình tối ưu hóa là Adam và độ đo hiệu suất là 'accuracy'\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # Trả về mô hình đã được xây dựng\n",
    "    return model\n",
    "\n",
    "# DATASET = \"Hiragana73\"\n",
    "DATASET = \"HiraganaGit\"\n",
    "\n",
    "#SWITCH THE LINES BELOW IF YOU NEED TO LOAD ALL THE DATA FROM THE DATASET AGAIN\n",
    "X, Y, imgPaths = loadDataset(DATASET, loadAgain=True)\n",
    "#X, Y, imgPaths = loadDataset(DATASET, loadAgain=False)\n",
    "\n",
    "X /= 255\n",
    "\n",
    "#X has format (height, width, N)\n",
    "\n",
    "indices = np.arange(X.shape[0])\n",
    "np.random.seed(3)\n",
    "np.random.shuffle(indices)\n",
    "X = X[indices]\n",
    "Y = Y[indices]\n",
    "imgPaths = imgPaths[indices]\n",
    "\n",
    "N = X.shape[0]\n",
    "\n",
    "TRAIN_RATIO = 20\n",
    "\n",
    "Ntrain = int(N*TRAIN_RATIO/100)\n",
    "Ntest = int(N*(100-TRAIN_RATIO)/100)\n",
    "\n",
    "x_train = X[:Ntrain].reshape((Ntrain, 1, 84, 83))\n",
    "y_train = Y[:Ntrain]\n",
    "paths_train = imgPaths[:Ntrain]\n",
    "\n",
    "x_test = X[Ntrain:Ntrain+Ntest].reshape((Ntest, 1, 84, 83))\n",
    "y_test = Y[Ntrain:Ntrain+Ntest]\n",
    "paths_test = imgPaths[Ntrain:Ntrain+Ntest]\n",
    "orig_y_test = np.copy(y_test)\n",
    "\n",
    "#CONVERT CLASSES TO NUMBERS\n",
    "\n",
    "labelToNumber = dict()\n",
    "numberToLabel = dict()\n",
    "counter = 0\n",
    "for i in range(len(y_train)):\n",
    "    if(y_train[i] not in labelToNumber):\n",
    "        labelToNumber[y_train[i]] = counter\n",
    "        numberToLabel[counter] = y_train[i]\n",
    "        counter += 1\n",
    "    y_train[i] = labelToNumber[y_train[i]]\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if(y_test[i] not in labelToNumber):\n",
    "        labelToNumber[y_test[i]] = counter\n",
    "        numberToLabel[counter]=y_test[i]\n",
    "        counter += 1\n",
    "    y_test[i] = labelToNumber[y_test[i]]\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "#NUMBER OF CLASSES\n",
    "num_classes = y_train.shape[1]\n",
    "num_epochs = 20\n",
    "\n",
    "trainAgain = True\n",
    "if trainAgain:\n",
    "    model = baseline_model()\n",
    "    metrics = model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs=num_epochs, batch_size=10) #returns val_loss, val_acc, loss, acc\n",
    "    modelJSON = model.to_json()\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(PATH+\"/../models/model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    #serialize training history\n",
    "    with open(PATH+\"/../models/history.pkl\", 'wb') as file_pi:\n",
    "        pkl.dump(metrics.history, file_pi)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(PATH+\"/../models/model.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "else:\n",
    "    model = loadModel()\n",
    "    y_pred_probs = model.predict(x_test)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    y_pred = [numberToLabel[char] for char in y_pred]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c039b817-e543-44cb-9bb2-5702a7e03bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdef8fc-60ea-485f-a0b3-01f374e2a7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd37a3b7-7148-4790-b992-0bec8c5d1eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
